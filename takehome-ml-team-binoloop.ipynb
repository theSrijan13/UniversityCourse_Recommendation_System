{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Challenge\n",
    "We are building a next semester course load LLM based recommender system for university students. \n",
    "Assume this will become a real tool to be used by real students some day.\n",
    "\n",
    "## Task\n",
    "\n",
    "There are 2 main objectives, in order of priority:\n",
    "1. Setup an evaluation system to test this LLM based recommender so we can understand where it performs well and where it fails. \n",
    "   - At a minimum, we expect that you'll create more synthetic data for this task and add it to the examples already in the data provided.\n",
    "   - We also expect you'll define some form of evaluation code to make sure that the recommender is behaving correctly.\n",
    "   - Correctness is a bit subjective here, at a minimum we'd like to avoid classes that clash in terms of time, classes for which the student does not have the pre-requisites and relevant classes given the student's course history, major and their request.\n",
    "   - Look through the data, think through what is realistic about it or not. Think of ways to combine programmatic and AI based solutions with synergy.\n",
    "\n",
    "2. Once you're done, you're to convert this code into an API with FastAPI. \n",
    "   - Include containerization using Dockerfile for the same.\n",
    "  \n",
    "3. [Optional] Make the recommender better based on what we learn from 1.\n",
    "   - Keep the recommender interface the same, but feel free to modify the prompt or add additional internal steps.\n",
    "\n",
    "## Data\n",
    "We've created a minimum set of data to get you started. This data includes a set of fake courses, majors and students.\n",
    "This is just a sample, most of this challenge is around generating and improving this data to make it more realistic.\n",
    "\n",
    "## Tools\n",
    "What's allowed? Literally anything apart from having someone else do this for you. \n",
    "You can use any tools you'd have regular access to in your work.\n",
    "\n",
    "You'll also get an Anyscale API key. Please be careful with this key, don't share it or commit it anywhere. We will get a notification if you do! You've been warned ðŸ˜‰.\n",
    "\n",
    "We recommend you take a look at the following libraries to help you:\n",
    "1. Together API\n",
    "2. Langchain\n",
    "3. Ragas / DeepEval\n",
    "\n",
    "## Submission\n",
    "Submit the final code to our team. We suggest adding comments to your code to explain your reasoning and to help us when we go through it in person later.\n",
    "\n",
    "## Evaluation\n",
    "- We want to see you flex your python muscles, write clean production quality code when you can.\n",
    "- We want to see how you think through this somewhat open-ended problem.\n",
    "  - Do you understand what's important when creating synthetic data and evaluating LLM powered products?\n",
    "  - Can you be biased towards action in the face of uncertainty and situations where there is no single right answer.\n",
    "- We want to see you problem solve in a real world environment, are you thoughtful and resourceful?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install langchain_openai==0.0.5 langchain_core==0.1.52 langchain==0.1.14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "# from langchain.cache import SQLiteCache\n",
    "from langchain.globals import set_llm_cache\n",
    "# from langchain_together import ChatTogether\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Setup LLM cache to speed things up (optional)\n",
    "# set_llm_cache(SQLiteCache(database_path=\".langchain.db\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Here are your dataclasses. Feel free to modify them if you want but you'll need to edit the data as well.\n",
    "@dataclass\n",
    "class Major:\n",
    "    name: str\n",
    "    required_courses: List[str]\n",
    "\n",
    "@dataclass\n",
    "class Course:\n",
    "    course_id: str\n",
    "    course_name: str\n",
    "    time_blocks: List[str]\n",
    "    prerequisites: List[str]\n",
    "    department: str\n",
    "    credits: int\n",
    "    level: str\n",
    "    instructor: str\n",
    "    description: str\n",
    "    selected_time_block: Optional[str] = None\n",
    "\n",
    "@dataclass\n",
    "class Student:\n",
    "    student_id: str\n",
    "    name: str\n",
    "    majors: List[Major]\n",
    "    completed_courses: List[str]\n",
    "    level: str\n",
    "    \n",
    "    def get_student_majors(self, all_majors):\n",
    "        return [all_majors[major_name] for major_name in self.majors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "import json\n",
    "with open('courses.json', 'r') as file:\n",
    "    courses_json = json.load(file)\n",
    "\n",
    "all_courses = [] \n",
    "for course_json in courses_json:\n",
    "    all_courses.append(Course(**course_json))\n",
    "    \n",
    "with open('students.json', 'r') as file:\n",
    "    students_json = json.load(file)\n",
    "\n",
    "all_students = [] \n",
    "for student_json in students_json:\n",
    "    all_students.append(Student(**student_json))\n",
    "\n",
    "with open('majors.json', 'r') as file:\n",
    "    majors_json = json.load(file)\n",
    "\n",
    "all_majors = {}\n",
    "for major_json in majors_json:\n",
    "    major = Major(**major_json)\n",
    "    all_majors[major.name] = major\n",
    "\n",
    "all_majors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 0: Set anyscale env key\n",
    "os.environ[\"TOGETHER_API_KEY\"] = 'e42468dadb57398b4575eca2eae2a9d516a519217a379bda5d02332aab6935c0' # This key is unique to you so do not share this anywhere else or you will be flagged.\n",
    "\n",
    "# Optional: If you have an openai key can also try using their models\n",
    "model = ChatOpenAI(base_url = 'https://api.together.xyz/v1', api_key=os.getenv('TOGETHER_API_KEY'), model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\")\n",
    "\n",
    "# Step 2: Create a prompt function that describes the task to the LLM\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Given the following student details, majors, and a list of possible courses, recommend which courses the student\n",
    "should take next semester. Consider prerequisites, major requirements, and avoid schedule conflicts.\n",
    "\\n\\nStudent Details: {student}\\n\\nMajors: {majors}\\n\\nPossible Courses: {possible_courses}.\n",
    "Student Request: {request}\n",
    "Your output should be a json in this format:\n",
    "{{\n",
    "    \"courses\": [\n",
    "        {{\"course_id\": \"abc123\", \"selected_time_slot\": \"A\"}},\n",
    "        {{\"course_id\": \"edf234\", \"selected_time_slot\": \"C\"}},\n",
    "        {{\"course_id\": \"fgh456\", \"selected_time_slot\": \"D\"}}\n",
    "        ...\n",
    "    ],\n",
    "    \"explanation\": \"<insert explanation of why this is a good course suggestion>\"\n",
    "}}\n",
    "\"\"\")\n",
    "\n",
    "# Step 3: Define a processing chain\n",
    "# This chain takes the student, their majors, and possible courses, generates a prompt, and gets recommendations from the LLM\n",
    "output_parser = StrOutputParser()\n",
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "student = all_students[3]\n",
    "student_request = 'I want to get as many courses from my major done as possible, but also have some diversity. I want to do 3 courses.'\n",
    "\n",
    "majors = student.get_student_majors(all_majors)\n",
    "recommendations = chain.invoke({'request': student_request, 'possible_courses': all_courses, 'student': student, 'majors': student.majors})\n",
    "\n",
    "print(recommendations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation (Your Task)\n",
    "Setup your evaluation code here\n",
    "\n",
    "Note: I've provided some inspiration code, but its not very good. Feel free to use it or delete it with better evaluations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Here is an example runner.\n",
    "Pros:\n",
    "- It runs several students and collects the right information so we can evaluate\n",
    "- It caches the output so it can be run multiple times quickly\n",
    "\n",
    "Cons:\n",
    "- It's sequential (inefficient)\n",
    "'''\n",
    "\n",
    "def get_recommendations(student):\n",
    "    majors = student.get_student_majors(all_majors)\n",
    "    recommendations = chain.invoke({\n",
    "        'request': student_request, \n",
    "        'possible_courses': all_courses, \n",
    "        'student': student, \n",
    "        'majors': majors\n",
    "    })\n",
    "    return recommendations\n",
    "\n",
    "recommendations = []\n",
    "errors = []\n",
    "for student in tqdm(all_students):\n",
    "    try:\n",
    "        recommendations.append({'student': student, 'recommendation': get_recommendations(student)})\n",
    "    except Exception as e:\n",
    "        errors.append({'student': student, 'error': str(e)})\n",
    "\n",
    "print(f'Successfully generated {len(recommendations)} / {(len(all_students))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Here is an example evaluation.\n",
    "Pros:\n",
    "- It evaluates the output on several relevant metrics\n",
    "\n",
    "Cons:\n",
    "- It's sequential (inefficient)\n",
    "- It produces text outputs instead of numerics, it's hard to go from this to a single \n",
    "metric we can use to decide if the model is working well or not\n",
    "- It doesn't test everything we want / care about\n",
    "- It uses an LLM to evaluate thing like course_time_slots overlaps that could be done programatically\n",
    "- It only evaluates one student at a time and provides no aggregate metrics\n",
    "'''\n",
    "\n",
    "from langchain.evaluation import load_evaluator\n",
    "from pprint import pprint\n",
    "# This is equivalent to loading using the enum\n",
    "from langchain.evaluation import EvaluatorType\n",
    "\n",
    "\n",
    "recommendation = recommendations[0]\n",
    "student = recommendation['student']\n",
    "query = prompt.invoke({'request': student_request, 'possible_courses': all_courses, 'student': student, 'majors': student.majors})\n",
    "prediction = recommendation['recommendation']\n",
    "\n",
    "\n",
    "# If you wanted to specify multiple criteria. Generally not recommended\n",
    "custom_criteria = {\n",
    "    \"course_relevancy\": \"Are the selected courses relevant for the student?\",\n",
    "    \"course_time_slots\": \"Are there time overlaps?\",\n",
    "    \"prerequisites\": \"Before taking the recommended classes, does the student have necessary prerequisites?\",\n",
    "    \"student_request\": \"Do the recommended courses follow the student request?\",\n",
    "}\n",
    "\n",
    "eval_chain = load_evaluator(\n",
    "    EvaluatorType.CRITERIA,\n",
    "    criteria=custom_criteria,\n",
    "    llm=model\n",
    ")\n",
    "eval_result = eval_chain.evaluate_strings(prediction=prediction, input=query)\n",
    "\n",
    "print(\"Recommendation\")\n",
    "pprint(recommendation)\n",
    "\n",
    "print(\"\\nMulti-criteria evaluation\")\n",
    "pprint(eval_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds-tech-interview-01",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
